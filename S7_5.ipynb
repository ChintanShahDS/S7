{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n09vaEgP6pLj"
      },
      "source": [
        "CODE BLOCK: 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df6R_VinBXGE"
      },
      "source": [
        "<!--\n",
        ">**This is Bold**\n",
        ">>*This is Italics*\n",
        ">>>- Simple statement\n",
        ">>> - `string`\n",
        ">>>> - $ mathematical symbols $\n",
        ">>>> 1. Number 1\n",
        ">>>> 2. Number 2\n",
        "\n",
        "\n",
        "&#x25BA;\n",
        "\n",
        "\n",
        "*** -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwPH_oyhBXGF"
      },
      "source": [
        "# Session 7 - Assignment\n",
        "## Basic expectations\n",
        "- \\> = 99.4% accuracy\n",
        "- < = 8000\n",
        "- < = 15 epochs\n",
        "\n",
        "## Step 5 - Image augmentation and adaptive LR scheduler\n",
        "\n",
        "### Target:\n",
        "- Look at images and use different image augmentations as per that to increase training complexity\n",
        "  - Center Crop: 24 size crop to remove the outside 4 pixels to remove some edges\n",
        "  - Image rotation: Checked and image rotation of +-20% seemed to be good\n",
        "  - Random perspective change\n",
        "  - Changes like brightness, contrast, saturation, hue, etc to have different types of images\n",
        "- Use adaptive LR scheduler to help with adjusting learning rate with patience=2 and relative threshold\n",
        "- Changed the batch size for training to 64 from 128 for it to backward propogate more times within same number of epochs\n",
        "\n",
        "### Results:\n",
        "- Epochs: 15\n",
        "- Parameters: 7,528\n",
        "- Training Batch size: 64\n",
        "- Testing Batch size: 2048\n",
        "- Training\n",
        "  - Loss=0.0037\n",
        "  - Batch_id=937\n",
        "  - Accuracy=98.36%\n",
        "- Testing\n",
        "  - Average loss: 0.0201\n",
        "  - Accuracy: 9941/10000 (99.41%)\n",
        "\n",
        "### Analysis:\n",
        "- Kept the params same\n",
        "- Accuracy has gone up and matching our target for couple of times (Not able to consistently get this on multiple runs - Saved this when got it once)\n",
        "- Did not see any overfitting\n",
        "- Need to either use some augmentations in data in addition to the current data to achieve consistency\n",
        "- Might have to look at different architectures / blocks that can help do the same with lesser number of parameters\n",
        "- But for now only asked to focus on the steps learned so trying from those (Tried many but not able to consistently get 99.4%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dsrOigmBXGG"
      },
      "source": [
        "### [1. Import appropriate modules required in this file](#section-1)\n",
        "- import model: importing the code written in model.py that is kept in path of current working directory\n",
        "- import utils: Similarly importing code in utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7FJ13JnGo0QI"
      },
      "outputs": [],
      "source": [
        "# !pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6PlbomWY3RSq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjBHHQVA6sXt"
      },
      "source": [
        "CODE BLOCK: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw9WOPNVBXGK"
      },
      "source": [
        "### [2. Check if GPU (CUDA) or CPU](#section-2)\n",
        "- Cuda availablbility check to ensure using GPU if available for the network\n",
        "- Taking the value into device variable if cuda (GPU) or only CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94BxVVBP3WwS",
        "outputId": "61c48a5e-aa72-4f28-c69d-6971204435bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ]
        }
      ],
      "source": [
        "SEED = 1\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if device==\"cuda\":\n",
        "    torch.cuda.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UHq59Sw6tmW"
      },
      "source": [
        "CODE BLOCK: 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83m4W4eLfBd"
      },
      "source": [
        "### [3. Data transforms for training and testing](#section-3)\n",
        "- Transforms specific to the training and testing\n",
        "- Training to include different transformations for the model to learn\n",
        "- Testing not to have the same to ensure validation is on normal set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OVfqiUyyLfBe"
      },
      "outputs": [],
      "source": [
        "# Train data transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomApply([\n",
        "      transforms.CenterCrop(size=(24,24)),\n",
        "      transforms.RandomRotation((-20.0, 20.0), fill=(1,)),\n",
        "      transforms.RandomPerspective(distortion_scale=0.3, interpolation=3, fill=0),\n",
        "      transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1)], p=0.2\n",
        "    ),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    ])\n",
        "\n",
        "# Test data transformations\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK3NQ6U6BXGM"
      },
      "source": [
        "### [4. Getting the MNIST train and test dataset from torchvision datasets library](#section-4)\n",
        "- Already separated datasets for train and test\n",
        "- train with *??* records and test with *10000* records\n",
        "- Marking train as True of False decides the train vs test data sets\n",
        "- The transforms are being loaded from utils file\n",
        "- transforms being done on train but not on test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JB79ZYW13-AO"
      },
      "outputs": [],
      "source": [
        "train = datasets.MNIST('../data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('../data', train=False, download=True, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PKSHxto6116"
      },
      "source": [
        "CODE BLOCK: 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjMoEkUJBXGY"
      },
      "source": [
        "### [5. Data loader setup for Training and Testing](#section-5)\n",
        "- Batch size definition - Keeping separate for Training and Testing\n",
        "- Training lower for better training and Testing higher for faster validation\n",
        "- Data loader definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "avCKK1uL4A68"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 64\n",
        "test_batch_size = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqXVzK3VBXGZ",
        "outputId": "26153c64-2be8-4db0-b045-7fb1d43a1107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "kwargs = {'shuffle': True, 'num_workers': 4, 'pin_memory': True}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=train_batch_size, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=test_batch_size, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi_0rfq56-29"
      },
      "source": [
        "CODE BLOCK: 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnrOq8wMBXGa"
      },
      "source": [
        "### [6. Data Visualization](#section-6)\n",
        "- Visualization of data to understand the images and corresponding labels\n",
        "- Also gives an understanding of the dataloader like Shuffle, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hx7QkLcw4Epc"
      },
      "outputs": [],
      "source": [
        "# dataiter = iter(train_loader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# print(images.shape)\n",
        "# print(labels.shape)\n",
        "\n",
        "# # Let's visualize some of the images\n",
        "# %matplotlib inline\n",
        "\n",
        "# batch_data, batch_label = next(iter(train_loader))\n",
        "\n",
        "# fig = plt.figure()\n",
        "\n",
        "# num_of_images = 60\n",
        "# for index in range(1, num_of_images + 1):\n",
        "#     plt.subplot(6, 10, index)\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09GYKBGRAT5M"
      },
      "source": [
        "CODE BLOCK: 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLVQ9d8PBXGb"
      },
      "source": [
        "### [7. Model training](#section-7)\n",
        "- Loading the model on the right device - CUDA or CPU\n",
        "- Setting up the right parameters like epochs, optimizer and scheduler for the training\n",
        "- Running the training and validating the output against Test dataset\n",
        "- While doing that also storing the loss and accuracy results for each epoch for understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7du4zM474LvT"
      },
      "outputs": [],
      "source": [
        "# Data to plot accuracy and loss graphs\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "test_incorrect_pred = {'images': [], 'ground_truths': [], 'predicted_vals': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q-8TQ7hipb0s"
      },
      "outputs": [],
      "source": [
        "debug = True\n",
        "dropout_value = 0.1\n",
        "# Our base model\n",
        "class Net3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net3, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 28\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 26\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 13\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 11\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock_t_0 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 11\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 9\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 7\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=24, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # output_size = 5\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock_t_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=24, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 5\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=5) # 7>> 9... nn.AdaptiveAvgPool((1, 1))\n",
        "        ) # output_size = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (debug == True):\n",
        "          print(f\"Start: {x.shape}\")\n",
        "        x = self.convblock0(x)  # channel size 28 > 28  RF 1 > 3    J 1 > 1\n",
        "        if (debug == True):\n",
        "          print(f\"Conv0: {x.shape}\")\n",
        "\n",
        "        x = self.convblock1(x)  # channel size 28 > 26  RF 3 > 5    J 1 > 1\n",
        "        if (debug == True):\n",
        "          print(f\"Conv1: {x.shape}\")\n",
        "        x = self.pool1(x)         # channel size 26 > 13  RF 5 > 6    J 1 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Pool1: {x.shape}\")\n",
        "        x = self.convblock2(x)  # channel size 13 > 11  RF 6 > 10    J 2 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Conv2: {x.shape}\")\n",
        "        x = self.convblock_t_0(x)  # channel size 11 > 11  RF 10 > 10    J 2 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Conv_t_0: {x.shape}\")\n",
        "\n",
        "        x = self.convblock3(x)  # channel size 11 > 9  RF 10 > 14    J 2 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Conv3: {x.shape}\")\n",
        "        x = self.convblock4(x)  # channel size 9 > 7  RF 14 > 18    J 2 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Conv4: {x.shape}\")\n",
        "        x = self.convblock5(x)  # channel size 7 > 5  RF 18 > 22    J 2 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Conv5: {x.shape}\")\n",
        "        x = self.convblock_t_2(x)  # channel size 5 > 5  RF 22 > 22    J 2 > 2\n",
        "        if (debug == True):\n",
        "          print(f\"Conv_t_2: {x.shape}\")\n",
        "\n",
        "        x = self.gap(x)\n",
        "        if (debug == True):\n",
        "          print(f\"GAP: {x.shape}\")\n",
        "        x = x.view(-1, 10)        # 2 * 2 * 10\n",
        "        if (debug == True):\n",
        "          print(f\"Flatten: {x.shape}\")\n",
        "\n",
        "        return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2t2GJ-fp4iw",
        "outputId": "7668488e-97cd-46b3-9823-c7c37feea328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start: torch.Size([2, 1, 28, 28])\n",
            "Conv3_0_1: torch.Size([2, 8, 28, 28])\n",
            "Conv3_1_1: torch.Size([2, 8, 26, 26])\n",
            "Pool1: torch.Size([2, 8, 13, 13])\n",
            "Conv3_1_2: torch.Size([2, 16, 11, 11])\n",
            "Conv1_1_4: torch.Size([2, 8, 11, 11])\n",
            "Conv3_2_1: torch.Size([2, 8, 9, 9])\n",
            "Conv3_2_2: torch.Size([2, 16, 7, 7])\n",
            "Conv3_2_2: torch.Size([2, 24, 5, 5])\n",
            "Conv1_2_4: torch.Size([2, 10, 5, 5])\n",
            "GAP: torch.Size([2, 10, 1, 1])\n",
            "Flatten: torch.Size([2, 10])\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              72\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "              ReLU-3            [-1, 8, 28, 28]               0\n",
            "           Dropout-4            [-1, 8, 28, 28]               0\n",
            "            Conv2d-5            [-1, 8, 26, 26]             576\n",
            "       BatchNorm2d-6            [-1, 8, 26, 26]              16\n",
            "              ReLU-7            [-1, 8, 26, 26]               0\n",
            "           Dropout-8            [-1, 8, 26, 26]               0\n",
            "         MaxPool2d-9            [-1, 8, 13, 13]               0\n",
            "           Conv2d-10           [-1, 16, 11, 11]           1,152\n",
            "      BatchNorm2d-11           [-1, 16, 11, 11]              32\n",
            "             ReLU-12           [-1, 16, 11, 11]               0\n",
            "          Dropout-13           [-1, 16, 11, 11]               0\n",
            "           Conv2d-14            [-1, 8, 11, 11]             128\n",
            "      BatchNorm2d-15            [-1, 8, 11, 11]              16\n",
            "             ReLU-16            [-1, 8, 11, 11]               0\n",
            "          Dropout-17            [-1, 8, 11, 11]               0\n",
            "           Conv2d-18              [-1, 8, 9, 9]             576\n",
            "      BatchNorm2d-19              [-1, 8, 9, 9]              16\n",
            "             ReLU-20              [-1, 8, 9, 9]               0\n",
            "          Dropout-21              [-1, 8, 9, 9]               0\n",
            "           Conv2d-22             [-1, 16, 7, 7]           1,152\n",
            "      BatchNorm2d-23             [-1, 16, 7, 7]              32\n",
            "             ReLU-24             [-1, 16, 7, 7]               0\n",
            "          Dropout-25             [-1, 16, 7, 7]               0\n",
            "           Conv2d-26             [-1, 24, 5, 5]           3,456\n",
            "      BatchNorm2d-27             [-1, 24, 5, 5]              48\n",
            "             ReLU-28             [-1, 24, 5, 5]               0\n",
            "          Dropout-29             [-1, 24, 5, 5]               0\n",
            "           Conv2d-30             [-1, 10, 5, 5]             240\n",
            "        AvgPool2d-31             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 7,528\n",
            "Trainable params: 7,528\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.52\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.55\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "mymodel = Net3().to(device)\n",
        "\n",
        "summary(mymodel, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DCamvSRWpyvL"
      },
      "outputs": [],
      "source": [
        "def GetCorrectPredCount(pPrediction, pLabels):\n",
        "  return pPrediction.argmax(dim=1).eq(pLabels).sum().item()\n",
        "\n",
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Predict\n",
        "    pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(pred, target)\n",
        "    train_loss+=loss.item()\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    correct += GetCorrectPredCount(pred, target)\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Train: Loss={loss.item():0.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "\n",
        "  train_acc= 100*correct/processed\n",
        "  print('\\nProcessed: {}, Len TrainLoader: {}'.format(processed, len(train_loader)))\n",
        "  # train_losses.append(train_loss/len(train_loader))\n",
        "  train_loss = train_loss/len(train_loader)\n",
        "  print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "      train_loss, correct, len(train_loader.dataset),\n",
        "      100. * correct / len(train_loader.dataset)))\n",
        "\n",
        "  return train_acc, train_loss\n",
        "\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target, reduction='sum').item()  # sum up batch loss\n",
        "\n",
        "            correct += GetCorrectPredCount(output, target)\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = (100. * correct / len(test_loader.dataset))\n",
        "    # test_losses.append(test_loss)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return test_acc, test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owqiet9M4TV7",
        "outputId": "ae22252b-6007-42a3-c051-2f30bcc63c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.1049 Batch_id=937 Accuracy=84.41: 100%|██████████| 938/938 [00:32<00:00, 28.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.5398, Accuracy: 50645/60000 (84.41%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0713, Accuracy: 9804/10000 (98.04%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0476 Batch_id=937 Accuracy=96.51: 100%|██████████| 938/938 [00:32<00:00, 29.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.1166, Accuracy: 57907/60000 (96.51%)\n",
            "Test set: Average loss: 0.0513, Accuracy: 9837/10000 (98.37%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.1389 Batch_id=937 Accuracy=97.31: 100%|██████████| 938/938 [00:32<00:00, 28.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0889, Accuracy: 58388/60000 (97.31%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0477, Accuracy: 9854/10000 (98.54%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0325 Batch_id=937 Accuracy=97.63: 100%|██████████| 938/938 [00:32<00:00, 28.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0775, Accuracy: 58578/60000 (97.63%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0421, Accuracy: 9868/10000 (98.68%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0081 Batch_id=937 Accuracy=97.77: 100%|██████████| 938/938 [00:32<00:00, 29.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0731, Accuracy: 58659/60000 (97.77%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0333, Accuracy: 9902/10000 (99.02%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0860 Batch_id=937 Accuracy=97.84: 100%|██████████| 938/938 [00:32<00:00, 28.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0692, Accuracy: 58705/60000 (97.84%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0301, Accuracy: 9905/10000 (99.05%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0584 Batch_id=937 Accuracy=97.93: 100%|██████████| 938/938 [00:34<00:00, 27.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0656, Accuracy: 58757/60000 (97.93%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0290, Accuracy: 9917/10000 (99.17%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0567 Batch_id=937 Accuracy=98.02: 100%|██████████| 938/938 [00:31<00:00, 29.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0640, Accuracy: 58810/60000 (98.02%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0296, Accuracy: 9909/10000 (99.09%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.1733 Batch_id=937 Accuracy=98.16: 100%|██████████| 938/938 [00:33<00:00, 28.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0603, Accuracy: 58893/60000 (98.16%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0233, Accuracy: 9925/10000 (99.25%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0145 Batch_id=937 Accuracy=98.20: 100%|██████████| 938/938 [00:32<00:00, 28.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0584, Accuracy: 58920/60000 (98.20%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0264, Accuracy: 9919/10000 (99.19%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0346 Batch_id=937 Accuracy=98.28: 100%|██████████| 938/938 [00:32<00:00, 29.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0559, Accuracy: 58967/60000 (98.28%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0228, Accuracy: 9936/10000 (99.36%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.1088 Batch_id=937 Accuracy=98.20: 100%|██████████| 938/938 [00:33<00:00, 28.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0570, Accuracy: 58919/60000 (98.20%)\n",
            "Test set: Average loss: 0.0237, Accuracy: 9926/10000 (99.26%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0321 Batch_id=937 Accuracy=98.25: 100%|██████████| 938/938 [00:33<00:00, 27.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0557, Accuracy: 58947/60000 (98.25%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0210, Accuracy: 9940/10000 (99.40%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0037 Batch_id=937 Accuracy=98.36: 100%|██████████| 938/938 [00:32<00:00, 29.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0536, Accuracy: 59019/60000 (98.36%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0201, Accuracy: 9941/10000 (99.41%)\n",
            "LR Rate: 0.01 \n",
            "\n",
            "Epoch 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train: Loss=0.0074 Batch_id=937 Accuracy=98.42: 100%|██████████| 938/938 [00:33<00:00, 28.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processed: 60000, Len TrainLoader: 938\n",
            "Train set: Average loss: 0.0508, Accuracy: 59054/60000 (98.42%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0241, Accuracy: 9917/10000 (99.17%)\n",
            "LR Rate: 0.01 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "debug = False\n",
        "optimizer = optim.SGD(mymodel.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, threshold_mode='rel', verbose=True)\n",
        "criterion = F.nll_loss\n",
        "num_epochs = 15\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  print(f'Epoch {epoch}')\n",
        "  train_acc, train_loss = train(mymodel, device, train_loader, optimizer, criterion)\n",
        "  test_acc, test_loss = test(mymodel, device, test_loader, criterion)\n",
        "  scheduler.step(test_loss)\n",
        "  print(\"LR Rate:\", optimizer.param_groups[0]['lr'], \"\\n\")\n",
        "  train_accs.append(train_acc)\n",
        "  train_losses.append(train_loss)\n",
        "  test_accs.append(test_acc)\n",
        "  test_losses.append(test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-LM-Z1k6FcF"
      },
      "source": [
        "CODE BLOCK: 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRTtZqmhBXGc"
      },
      "source": [
        "### [8. Model Accuracy and Loss plots visualization](#section-8)\n",
        "- Part of utils\n",
        "- Drawing plots of Training and Testing accuracy and loss\n",
        "- This gives us an understanding of how the accuracy and losses improved over epochs\n",
        "- Also helps to understand where we should have stopped the training for optimum results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu0l7dli4eC9"
      },
      "outputs": [],
      "source": [
        "# utils.drawLossAccuracyPlots(train_losses, train_accs, test_losses, test_accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiuPNnuoFOOd"
      },
      "outputs": [],
      "source": [
        "# fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "# axs[0, 0].plot(train_losses)\n",
        "# axs[0, 0].set_title(\"Training Loss\")\n",
        "# axs[1, 0].plot(train_accs)\n",
        "# axs[1, 0].set_title(\"Training Accuracy\")\n",
        "# axs[0, 1].plot(test_losses)\n",
        "# axs[0, 1].set_title(\"Test Loss\")\n",
        "# axs[1, 1].plot(test_accs)\n",
        "# axs[1, 1].set_title(\"Test Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyKtJ6jw-IhU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
